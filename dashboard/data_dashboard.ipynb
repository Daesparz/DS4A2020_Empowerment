{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: Dashboard Compilation\n",
    "\n",
    "This notebook compiles the information required to build the dashboard of each metropolitan area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import dashboard_functions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import os\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/interim/census/household_shopping_concat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.read_csv('../data/processed/shopping_statistics_msa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GENERATION'] = df.TBIRTH_YEAR.apply(dashboard_functions.generation_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.rename({'CHNGHOW4': 'RFID-NFC', \n",
    "                   'CHNGHOW5': 'CASH', \n",
    "                   'CHNGHOW6': 'AVOID RESTR',\n",
    "                   'CHNGHOW7': 'RESUME RESTR'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSA_codes = list(df_summary.EST_MSA)\n",
    "MSA_names = list(df_summary.MSA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.EST_MSA.isin(MSA_codes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between Consumer variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[:, ['EST_MSA', 'CHNGHOW1', 'CHNGHOW2', 'CHNGHOW3', 'CHNGHOW4', \n",
    "                'CHNGHOW5', 'CHNGHOW6', 'CHNGHOW7']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename({'CHNGHOW1': 'ONLINE',\n",
    "           'CHNGHOW2': 'PICK-UP',\n",
    "           'CHNGHOW3': 'IN-STORE',\n",
    "           'CHNGHOW4': 'RFID-NFC', \n",
    "           'CHNGHOW5': 'CASH', \n",
    "           'CHNGHOW6': 'AVOID RESTR',\n",
    "           'CHNGHOW7': 'RESUME RESTR'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in range(len(MSA_codes)):\n",
    "#     dashboard_functions.shopping_behaviors_correlation(df1, MSA_codes[index], MSA_names[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each Consumer variable against demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shopping_variables = ['CHNGHOW1', 'CHNGHOW2', 'CHNGHOW3', 'CHNGHOW4', 'CHNGHOW5', 'CHNGHOW6', 'CHNGHOW7']\n",
    "# name_shopping_variables = ['ONLINE','PICK-UP','IN-STORE','RFID-NFC','CASH','AVOID RESTR','RESUME RESTR']\n",
    "\n",
    "# for i in range(len(MSA_codes)):\n",
    "#     for j in range(len(shopping_variables)):\n",
    "#         dashboard_functions.cramers_matrix(df, MSA_codes[i], \n",
    "#                                            MSA_names[i], \n",
    "#                                            shopping_variables[j], \n",
    "#                                            name_shopping_variables[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Mobility Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descartes_counties = pd.read_csv('../data/interim/mobility/m50_max_counties.csv')\n",
    "df_descartes_counties_percent = pd.read_csv('../data/interim/mobility/m50_percent_counties.csv')\n",
    "\n",
    "df_apple = pd.read_csv('../data/interim/mobility/apple_mobility_cities.csv', index_col=0)\n",
    "\n",
    "df_foursquare_dma = pd.read_csv('../data/interim/mobility/foursquare_dma.csv', index_col=0)\n",
    "df_google = pd.read_csv('../data/interim/mobility/google_counties.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Atlanta-Sandy Springs-Alpharetta',\n",
       " 'Boston-Cambridge-Newton',\n",
       " 'Chicago-Naperville-Elgin',\n",
       " 'Dallas-Fort Worth-Arlington',\n",
       " 'Detroit-Warren-Dearborn',\n",
       " 'Houston-The Woodlands-Sugar Land',\n",
       " 'Los Angeles-Long Beach-Anaheim',\n",
       " 'Miami-Fort Lauderdale-Pompano Beach',\n",
       " 'New York-Newark-Jersey City',\n",
       " 'Philadelphia-Camden-Wilmington',\n",
       " 'Phoenix-Mesa-Chandler',\n",
       " 'Riverside-San Bernardino-Ontario',\n",
       " 'San Francisco-Oakland-Berkeley',\n",
       " 'Seattle-Tacoma-Bellevue',\n",
       " 'Washington-Arlington-Alexandria']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSA_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexed_mobility_average_person(df, location):\n",
    "    df_index = df[df.NAME == location].loc[:, '2020-03-01':]\n",
    "\n",
    "    df_index = pd.melt(df_index, var_name='date', value_name='value')\n",
    "    df_index['date'] = pd.to_datetime(df_index['date'])\n",
    "    df_index['location'] = location\n",
    "    df_index['src'] = 'indexed regular member'\n",
    "    \n",
    "    return df_index.loc[:, ['date', 'location', 'src', 'value']]\n",
    "\n",
    "def mobility_average_person(df, location):\n",
    "    df_kms= df[df.NAME == location].loc[:, '2020-03-01':]\n",
    "\n",
    "    df_kms = pd.melt(df_kms, var_name='date', value_name='value')\n",
    "    df_kms['date'] = pd.to_datetime(df_kms['date'])\n",
    "    df_kms['location'] = location   \n",
    "    df_kms['src'] = 'kms regular member'\n",
    "    \n",
    "    return df_kms.loc[:, ['date', 'location', 'src', 'value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_locations = {'Miami-Fort Lauderdale-Pompano Beach': ['Miami-Dade County', 'Broward County', 'Palm Beach County'],\n",
    "#                   'San Francisco-Oakland-Berkeley': ['San Francisco County', 'Alameda County', 'Marin County', \n",
    "#                                                      'Contra Costa County', 'San Mateo County'], \n",
    "#                   'Dallas-Fort Worth-Arlington': ['Collin County', 'Dallas County', 'Denton County', 'Ellis County', \n",
    "#                                                   'Hunt County', 'Kaufman County', 'Rockwall County', 'Johnson County',\n",
    "#                                                   'Parker County','Tarrant County','Wise County']\n",
    "#                   'Los Angeles-Long Beach-Anaheim': ['Ventura County', 'San Bernardino County', 'Riverside County',\n",
    "#                                                      'Los Angeles County', 'Orange County'],\n",
    "#                   'Atlanta-Sandy Springs-Alpharetta': [],\n",
    "#                   'Boston-Cambridge-Newton': [],\n",
    "#                   'Chicago-Naperville-Elgin': [],\n",
    "#                   'Detroit-Warren-Dearborn': [],\n",
    "#                   'Houston-The Woodlands-Sugar Land': [],\n",
    "#                   'Los Angeles-Long Beach-Anaheim': [],\n",
    "#                   'New York-Newark-Jersey City': [],\n",
    "#                   'Philadelphia-Camden-Wilmington': [],\n",
    "#                   'Phoenix-Mesa-Chandler': [],\n",
    "#                   'Riverside-San Bernardino-Ontario': [],\n",
    "#                   'Seattle-Tacoma-Bellevue': [],\n",
    "#                   'Washington-Arlington-Alexandria': []\n",
    "\n",
    "#                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['Miami-Dade County',\n",
    "             'San Francisco County']\n",
    "df_index = pd.DataFrame()\n",
    "df_kms = pd.DataFrame()\n",
    "\n",
    "for i, location in enumerate(locations):\n",
    "    temp1 = indexed_mobility_average_person(df_descartes_counties, location)\n",
    "    temp2  = mobility_average_person(df_descartes_counties_percent, location)\n",
    "    if i == 0:\n",
    "        df_index = temp1\n",
    "        df_kms = temp2\n",
    "    else:\n",
    "        df_index = pd.concat([df_index, temp1], ignore_index=True)\n",
    "        df_kms = pd.concat([df_kms, temp2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobility_venues_foursquare(df, location):\n",
    "    #Locations: Los Angeles, New York, SanFrancisco-Oakland-SanJose, Seattle-Tacoma\n",
    "    try:\n",
    "        df = df[[location,'class']]\n",
    "    except:\n",
    "        print('this location does not exist. Select one between the following list: \\n1. Los Angeles \\n2. New York \\n3. SanFrancisco-Oakland-SanJose \\n4. Seattle-Tacoma')\n",
    "        return None\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    temp = df.groupby('class').count().reset_index()\n",
    "    max_data = max(temp[location])\n",
    "    #only display historical data with 80% of the longer serie\n",
    "    valid_venues = list(temp[temp[location] >= 0.8*max_data]['class'])\n",
    "    df_output = df[df['class'].isin(valid_venues)].reset_index().rename({' ':'date'}, axis=1)\n",
    "    df_output['location'] = location\n",
    "    df_output['source'] = 'venues foursquare'\n",
    "    df_output['value'] = df_output[location]\n",
    "\n",
    "    return df_output.loc[:, ['date', 'location', 'source', 'value', 'class']]\n",
    "\n",
    "def mobility_venues_google(df, location):\n",
    "    df = df[df.NAME == location]\n",
    "    df1 = pd.melt(df, id_vars=['NAME', 'category'], var_name='Date')\n",
    "    df1.Date = pd.to_datetime(df1.Date)\n",
    "    temp = df1.groupby('category').count().reset_index()\n",
    "\n",
    "    max_data = max(temp['value'])\n",
    "    #only display historical data with 80% of the longer serie\n",
    "    valid_venues = list(temp[temp['value'] >= 0.8*max_data]['category'])\n",
    "    \n",
    "    df_output = df1[df1['category'].isin(valid_venues)].reset_index().rename({'Date':'date', 'category': 'class'}, axis=1)\n",
    "    df_output.drop(columns={'index'}, inplace=True) \n",
    "    df_output['location'] = location\n",
    "    df_output['source'] = 'venues google'\n",
    "    \n",
    "    return df_output.loc[:, ['date', 'location', 'source', 'value', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['Miami-Dade County', 'San Francisco County', ]\n",
    "venues_google = pd.DataFrame()\n",
    "\n",
    "for i, location in enumerate(locations):\n",
    "    temp1 = mobility_venues_google(df_google, location)\n",
    "    if i == 0:\n",
    "        venues_google = temp1\n",
    "    else:\n",
    "        venues_google = pd.concat([venues_google, temp1], ignore_index=True)\n",
    "        \n",
    "locations = ['SanFrancisco-Oakland-SanJose']\n",
    "venues_foursquare = pd.DataFrame()\n",
    "\n",
    "for i, location in enumerate(locations):\n",
    "    temp2  = mobility_venues_foursquare(df_foursquare_dma, location)\n",
    "    if i == 0:\n",
    "        venues_foursquare = temp2\n",
    "    else:\n",
    "        venues_foursquare = pd.concat([venues_foursquare, temp2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serie_decomposition(df, colname, model, display, location):\n",
    "\n",
    "    df = df[df.city == location]\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    if df[colname].isnull().any():\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "    result_add = seasonal_decompose(df[colname], model=model, extrapolate_trend='freq')\n",
    "    df_reconstructed = pd.concat([result_add.seasonal, result_add.trend, result_add.resid, result_add.observed], axis=1)\n",
    "    df_reconstructed.columns = ['seas', 'trend', 'resid', 'actual_values']\n",
    "        \n",
    "    return df_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['New York City', 'Los Angeles', 'Houston', 'Detroit', 'Dallas',\n",
       "       'Boston', 'Philadelphia', 'Miami', 'Chicago', 'San Diego',\n",
       "       'Seattle', 'San Francisco - Bay Area'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_apple.city.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomposition_selection(df, signal):\n",
    "    if signal in ['seas', 'trend', 'resid', 'actual_values']:\n",
    "        pass\n",
    "    else:\n",
    "        return None\n",
    "    df_transit = pd.DataFrame()\n",
    "    df_driving = pd.DataFrame()\n",
    "    df_walking = pd.DataFrame()\n",
    "\n",
    "    for i, location in enumerate(list(df.city.unique())):\n",
    "        out1 = serie_decomposition(df, 'trn', 'additive', False, location)\n",
    "        out2 = serie_decomposition(df, 'drv', 'additive', False, location)\n",
    "        out3 = serie_decomposition(df, 'wlk', 'additive', False, location)\n",
    "\n",
    "        out1 = out1.reset_index()\n",
    "        out1['src'] = 'transit '+ signal\n",
    "        out1['location'] = location\n",
    "        out1['value'] = out1[signal]\n",
    "\n",
    "        out2 = out2.reset_index()\n",
    "        out2['src'] = 'driving '+ signal\n",
    "        out2['location'] = location\n",
    "        out2['value'] = out2[signal]\n",
    "\n",
    "        out3 = out3.reset_index()\n",
    "        out3['src'] = 'walking '+ signal\n",
    "        out3['location'] = location\n",
    "        out3['value'] = out3[signal]\n",
    "\n",
    "        if i == 0:\n",
    "            df_transit = out1\n",
    "            df_driving = out2\n",
    "            df_walking = out3\n",
    "        else:\n",
    "            df_transit = pd.concat([df_transit, out1], ignore_index=True)\n",
    "            df_driving = pd.concat([df_driving, out2], ignore_index=True)\n",
    "            df_walking = pd.concat([df_walking, out3], ignore_index=True)\n",
    "            \n",
    "            \n",
    "    df_transit = df_transit.loc[:, ['date', 'location', 'src', 'value']]\n",
    "    df_driving = df_driving.loc[:, ['date', 'location', 'src', 'value']]\n",
    "    df_walking = df_walking.loc[:, ['date', 'location', 'src', 'value']]\n",
    "            \n",
    "    return df_transit, df_driving, df_walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame()\n",
    "df_drn = pd.DataFrame()\n",
    "df_wlk = pd.DataFrame()\n",
    "\n",
    "for signal in ['seas', 'trend', 'resid', 'actual_values']:\n",
    "    trn, drv, wlk = decomposition_selection(df_apple, signal)\n",
    "    if i == 0:\n",
    "        df_trn = trn\n",
    "        df_drn = drv\n",
    "        df_wlk = wlk\n",
    "    else:\n",
    "        df_trn = pd.concat([df_trn, trn], ignore_index=True)\n",
    "        df_drn = pd.concat([df_drn, drv], ignore_index=True)\n",
    "        df_wlk = pd.concat([df_wlk, wlk], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mobility_indexed = pd.concat([df_index, df_kms, df_trn, df_drn, df_wlk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mobility_indexed.to_csv('../data/processed/dashboard_mobility_indexed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foot_traffic = pd.concat([venues_foursquare, venues_google])\n",
    "df_foot_traffic.to_csv('../data/processed/dashboard_foot_traffic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Restaurants\n",
    "\n",
    "The following analysis includes 50 businesses/every zip code of San Francisco county. The information of the restaurants was extracted using FUSION API Yelp, which allows a maximum of 50 results for endpoint using the zip codes as keywords and words as Restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants_sf = pd.read_csv('../data/interim/restaurants/yelp_sf.csv')\n",
    "df_restaurants_miami = pd.read_csv('../data/interim/restaurants/yelp_miami.csv')\n",
    "df_restaurants_dallas = pd.read_csv('../data/interim/restaurants/yelp_dallas.csv')\n",
    "df_restaurants_la = pd.read_csv('../data/interim/restaurants/yelp_losAngeles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df_restaurants = [df_restaurants_sf, \n",
    "                       df_restaurants_miami, \n",
    "                       df_restaurants_dallas,\n",
    "                       df_restaurants_la]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants = pd.DataFrame()\n",
    "\n",
    "for i, df in enumerate(list_df_restaurants):\n",
    "    try:\n",
    "        df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    df['purchase'] = df['transactions'].apply(dashboard_functions.transaction_cleaning)\n",
    "    df['pickup_delivery'] = df['purchase'].apply(dashboard_functions.check_pickup_delivery)\n",
    "    if i == 0:\n",
    "        df_restaurants = df\n",
    "    else:\n",
    "        df_restaurants = pd.concat([df_restaurants, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.to_csv('../data/processed/dashboard_yelp_restaurants.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
